{"cells":[{"cell_type":"markdown","metadata":{"id":"VcQufrWvfY1J"},"source":["## Google Colab setup (don't run locally)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52505,"status":"ok","timestamp":1744841534922,"user":{"displayName":"Jade S","userId":"08462326512389871838"},"user_tz":240},"id":"8ld1DehvfmVE","outputId":"6598c946-2278-43c2-9ce5-26f5b309bc94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Dataset URL: https://www.kaggle.com/datasets/xhlulu/leafsnap-dataset\n","License(s): copyright-authors\n"]}],"source":["from google.colab import drive\n","import os\n","import shutil\n","\n","# Mount drive to colab\n","drive.mount('/content/drive', force_remount=True)\n","\n","# setting up paths\n","path_to_project_files = '/content/drive/MyDrive/School/Homework/Spring2025/DL/Project/'\n","existing = os.path.join(path_to_project_files, 'kaggle.json')\n","path_to_colab_utils = '/root/.kaggle'\n","target = os.path.join(path_to_colab_utils, 'kaggle.json')\n","\n","# move the key to the colab root\n","os.makedirs(path_to_colab_utils, exist_ok=True)\n","shutil.copy(existing, target)\n","os.chmod(target, 600)\n","\n","# download the data into /content (which is temporary)\n","!kaggle datasets download -d xhlulu/leafsnap-dataset -p /content --unzip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pojg5BTgiA2x"},"outputs":[],"source":["import sys\n","\n","# Edit this path to where you've uploaded the repo files, so the imports work.\n","sys.path.append('/content/drive/MyDrive/School/Homework/Spring2025/DL/Project/')"]},{"cell_type":"markdown","metadata":{"id":"VZ6t5flAg_L3"},"source":["## Library Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2wCOwrEg_L7"},"outputs":[],"source":["from autoencoder import *\n","from dataloader import *\n","from cnn import *\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import v2\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_vHlyC5Vg_L8"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def showTensorInNotebook(tensor):\n","    \"\"\"\n","    This takes a (3[RGB], H, W) tensor in R[0.0, 1.0] and displays it with matplotlib.\n","    \"\"\"\n","    image = tensor.detach().cpu().numpy().transpose(1,2,0) # move the channel axis to the end, because PIL and matplotlib hate each other\n","    plt.imshow(image)\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58,"status":"ok","timestamp":1743259183321,"user":{"displayName":"Jade S","userId":"08462326512389871838"},"user_tz":240},"id":"qlvsRzNOrXsT","outputId":"225f2daf-3632-4e3e-c82f-e4bdb3792755"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')"]},{"cell_type":"markdown","metadata":{"id":"B17GtXW4g_L9"},"source":["## Building the data loader"]},{"cell_type":"markdown","metadata":{"id":"Snd7PueTg_L9"},"source":["These are transforms that allow us to ingest the image tensors with some extra confusion at training time. `processor` makes the data loader spit out tensors, and `noiser` adds Gaussian noise."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZBLCG5lig_L9"},"outputs":[],"source":["# This just processes the images.\n","NOISE_RATIO = 0.1\n","H, W = 256, 256\n","\n","processor = v2.Compose([\n","    v2.PILToTensor(), # the LeafsnapDataset class gives PIL Images, convert to torch Tensor\n","    v2.Resize((H, W)), # resize\n","    v2.RandomHorizontalFlip(),\n","    v2.RandomVerticalFlip(),\n","    v2.RandomRotation(degrees=(-30, 30)),\n","    lambda x: x / 255.0, # convert N[0, 255] to R[0.0, 1.0]\n","    lambda x: torch.clip(x + NOISE_RATIO*torch.randn_like(x), 0.0, 1.0), # add noise\n","])\n","\n","lab_processor = v2.Compose([\n","    v2.PILToTensor(), # the LeafsnapDataset class gives PIL Images, convert to torch Tensor\n","    lambda x: v2.functional.crop(x, 0, 0, 600, 600),\n","    v2.Resize((H, W)), # resize\n","    v2.RandomHorizontalFlip(),\n","    v2.RandomVerticalFlip(),\n","    v2.RandomRotation(degrees=(-30, 30)),\n","    lambda x: x / 255.0, # convert N[0, 255] to R[0.0, 1.0]\n","    lambda x: torch.clip(x + NOISE_RATIO*torch.randn_like(x), 0.0, 1.0), # add noise\n","])\n","\n","test_processor = v2.Compose([\n","    v2.PILToTensor(), # the LeafsnapDataset class gives PIL Images, convert to torch Tensor\n","    v2.Resize((H, W)), # resize\n","    lambda x: x / 255.0, # convert N[0, 255] to R[0.0, 1.0]\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"frUcSFc1g_L-"},"outputs":[],"source":["BATCH_SIZE = 32\n","\n","root_directory = os.path.join(os.getcwd(), 'leafsnap-dataset') # you make need to edit this path to work, though, it works on Colab by default and works locally if you keep the dataset at the root of the repo\n","train_image_paths_file = os.path.join(path_to_project_files, \"train.txt\")\n","train_dataset = LeafsnapDataset(train_image_paths_file, root_directory, use_segmented=False, source=\"both\", transform=processor, lab_transform=lab_processor)\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","validation_image_paths_file = os.path.join(path_to_project_files, \"validation.txt\")\n","validation_dataset = LeafsnapDataset(validation_image_paths_file, root_directory, use_segmented=False, source=\"both\", transform=processor, lab_transform=lab_processor)\n","validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","test_image_paths_file = os.path.join(path_to_project_files, \"test.txt\")\n","test_dataset = LeafsnapDataset(test_image_paths_file, root_directory, use_segmented=False, source=\"both\", transform=test_processor, lab_transform=lab_processor)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"FaLBO6h8g_L-"},"source":["## Training the Convolutional Neural Network (CNN)"]},{"cell_type":"markdown","metadata":{"id":"WK2Yd5Nwg_L_"},"source":["This model uses a modified version of ResNet from Homework 2. It specifically is a version of ResNet34, with kernel size raised to 5, and skip layers at sizes 32, 64, and 128. Images have been downscaled to 128x128, and the segmentation image is used as a 4th layer, resulting in an input size of 4x128x128."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"693oRWNdg_L_","executionInfo":{"status":"ok","timestamp":1743270100981,"user_tz":240,"elapsed":10917177,"user":{"displayName":"Jade S","userId":"08462326512389871838"}},"outputId":"69039436-3354-43f1-d072-d447175f02c7"},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/20: 100%|██████████| 342/342 [08:33<00:00,  1.50s/it, batch=342/342, loss=3.99]\n","Test Epoch 1/20: 100%|██████████| 43/43 [00:33<00:00,  1.28it/s, batch=43/43, test_loss=3.29]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 loss: 3.6492191650714094, val loss: 3.7890299420024074\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/20: 100%|██████████| 342/342 [08:32<00:00,  1.50s/it, batch=342/342, loss=4.04]\n","Test Epoch 2/20: 100%|██████████| 43/43 [00:34<00:00,  1.26it/s, batch=43/43, test_loss=2.93]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 loss: 3.3935030252612823, val loss: 3.307366836902707\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/20: 100%|██████████| 342/342 [08:33<00:00,  1.50s/it, batch=342/342, loss=3.82]\n","Test Epoch 3/20: 100%|██████████| 43/43 [00:33<00:00,  1.28it/s, batch=43/43, test_loss=2.96]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 loss: 3.18974488869048, val loss: 3.599104886831239\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/20: 100%|██████████| 342/342 [08:32<00:00,  1.50s/it, batch=342/342, loss=3.2]\n","Test Epoch 4/20: 100%|██████████| 43/43 [00:33<00:00,  1.27it/s, batch=43/43, test_loss=4.43]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 loss: 3.0089794355526305, val loss: 4.3354515752127005\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/20: 100%|██████████| 342/342 [08:33<00:00,  1.50s/it, batch=342/342, loss=3.44]\n","Test Epoch 5/20: 100%|██████████| 43/43 [00:34<00:00,  1.26it/s, batch=43/43, test_loss=2.65]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 loss: 2.8740871837961746, val loss: 3.034803723180017\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/20: 100%|██████████| 342/342 [08:33<00:00,  1.50s/it, batch=342/342, loss=3.64]\n","Test Epoch 6/20: 100%|██████████| 43/43 [00:33<00:00,  1.28it/s, batch=43/43, test_loss=4.67]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6 loss: 2.7342682581895974, val loss: 3.629049905510836\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/20: 100%|██████████| 342/342 [08:34<00:00,  1.51s/it, batch=342/342, loss=3.1]\n","Test Epoch 7/20: 100%|██████████| 43/43 [00:34<00:00,  1.25it/s, batch=43/43, test_loss=2.82]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7 loss: 2.6223694357955667, val loss: 3.14322558114695\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/20: 100%|██████████| 342/342 [08:32<00:00,  1.50s/it, batch=342/342, loss=3.24]\n","Test Epoch 8/20: 100%|██████████| 43/43 [00:33<00:00,  1.28it/s, batch=43/43, test_loss=3.5]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8 loss: 2.518923728786714, val loss: 3.434870819712794\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/20: 100%|██████████| 342/342 [08:33<00:00,  1.50s/it, batch=342/342, loss=2.25]\n","Test Epoch 9/20: 100%|██████████| 43/43 [00:34<00:00,  1.26it/s, batch=43/43, test_loss=3.7]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 loss: 2.4048568463464925, val loss: 2.7042835618174355\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/20: 100%|██████████| 342/342 [08:32<00:00,  1.50s/it, batch=342/342, loss=4.77]\n","Test Epoch 10/20: 100%|██████████| 43/43 [00:32<00:00,  1.31it/s, batch=43/43, test_loss=4.24]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10 loss: 2.3295940802111264, val loss: 3.9410534792168197\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/20: 100%|██████████| 342/342 [08:30<00:00,  1.49s/it, batch=342/342, loss=3.07]\n","Test Epoch 11/20: 100%|██████████| 43/43 [00:33<00:00,  1.29it/s, batch=43/43, test_loss=3.05]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11 loss: 2.234630935721927, val loss: 2.6116063511648844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/20: 100%|██████████| 342/342 [08:29<00:00,  1.49s/it, batch=342/342, loss=2.68]\n","Test Epoch 12/20: 100%|██████████| 43/43 [00:33<00:00,  1.28it/s, batch=43/43, test_loss=3.34]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12 loss: 2.1410595778136225, val loss: 3.0183202555013255\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/20: 100%|██████████| 342/342 [08:30<00:00,  1.49s/it, batch=342/342, loss=2.52]\n","Test Epoch 13/20: 100%|██████████| 43/43 [00:33<00:00,  1.30it/s, batch=43/43, test_loss=3.17]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13 loss: 2.0678655613235564, val loss: 2.430154575858005\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/20: 100%|██████████| 342/342 [08:30<00:00,  1.49s/it, batch=342/342, loss=2.09]\n","Test Epoch 14/20: 100%|██████████| 43/43 [00:33<00:00,  1.27it/s, batch=43/43, test_loss=5.65]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14 loss: 2.0056070758585345, val loss: 4.480227176533189\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/20: 100%|██████████| 342/342 [08:32<00:00,  1.50s/it, batch=342/342, loss=2.84]\n","Test Epoch 15/20: 100%|██████████| 43/43 [00:34<00:00,  1.26it/s, batch=43/43, test_loss=2.39]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15 loss: 1.9230773759167097, val loss: 2.154624112816744\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16/20: 100%|██████████| 342/342 [08:30<00:00,  1.49s/it, batch=342/342, loss=2.18]\n","Test Epoch 16/20: 100%|██████████| 43/43 [00:32<00:00,  1.32it/s, batch=43/43, test_loss=1.59]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16 loss: 1.8566918700759174, val loss: 1.9099135703818744\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17/20: 100%|██████████| 342/342 [08:32<00:00,  1.50s/it, batch=342/342, loss=2.14]\n","Test Epoch 17/20: 100%|██████████| 43/43 [00:34<00:00,  1.25it/s, batch=43/43, test_loss=2.7]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17 loss: 1.8014644802662365, val loss: 3.5328893606052842\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18/20: 100%|██████████| 342/342 [08:32<00:00,  1.50s/it, batch=342/342, loss=2.39]\n","Test Epoch 18/20: 100%|██████████| 43/43 [00:33<00:00,  1.30it/s, batch=43/43, test_loss=5.21]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18 loss: 1.739818641316821, val loss: 4.548547983169556\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19/20: 100%|██████████| 342/342 [08:30<00:00,  1.49s/it, batch=342/342, loss=2.35]\n","Test Epoch 19/20: 100%|██████████| 43/43 [00:33<00:00,  1.30it/s, batch=43/43, test_loss=2.16]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19 loss: 1.6909921291278818, val loss: 1.9544205970542377\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20/20: 100%|██████████| 342/342 [08:29<00:00,  1.49s/it, batch=342/342, loss=3.74]\n","Test Epoch 20/20: 100%|██████████| 43/43 [00:33<00:00,  1.27it/s, batch=43/43, test_loss=2.28]"]},{"output_type":"stream","name":"stdout","text":["Epoch 20 loss: 1.6382464541677844, val loss: 1.8368195711180222\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["model = resnet(3, 185, device=device)\n","model.load_state_dict(torch.load(os.path.join(path_to_project_files, 'cnn_model.pth'), weights_only=True))\n","\n","train_resnet_model(model, train_dataloader, validation_dataloader, 20, .001, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQzs2bGwg_MA"},"outputs":[],"source":["torch.save(model.state_dict(), path_to_project_files + \"cnn_model.pth\")"]},{"cell_type":"markdown","metadata":{"id":"rVA0iHLeg_MA"},"source":["## Testing the CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"whBbgkVbg_MB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743270130487,"user_tz":240,"elapsed":29410,"user":{"displayName":"Jade S","userId":"08462326512389871838"}},"outputId":"d0662fc8-f829-4e80-e863-7195dde0c6ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Top-1 Accuracy: 50.91%\n","Top-5 Accuracy: 83.61%\n"]}],"source":["correct_top1 = 0\n","correct_top5 = 0\n","total = 0\n","\n","model.to(device)\n","with torch.no_grad(): # No gradients needed for evaluation\n","    model.eval()\n","    for inputs, labels in test_dataloader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","\n","        # Top-1 Accuracy\n","        _, predicted = torch.max(outputs, 1)\n","        correct_top1 += (predicted == labels).sum().item()\n","\n","        # Top-5 Accuracy\n","        top5_preds = torch.topk(outputs, 5, dim=1).indices\n","        correct_top5 += torch.sum(top5_preds.eq(labels.view(-1, 1))).item()\n","\n","        total += labels.size(0)\n","\n","# Compute accuracies\n","top1_accuracy = 100 * correct_top1 / total\n","top5_accuracy = 100 * correct_top5 / total\n","\n","print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\n","print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"b_t7sGbPWNOn"},"source":["After various tweaks, I'm very happy with the current training accuracy of the CNN model, especially for the first check-in. Running at a 90% Top-5 accuracy is excellent, although there is certainly some more hyperparameter tweaking to be done. I may also test changing the model's structure, adding techniques such as dropout that have been used in other models for similar purposes. I would like to reach 80% Top-1 accuracy by the end."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"}},"nbformat":4,"nbformat_minor":0}