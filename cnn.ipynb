{"cells":[{"cell_type":"markdown","metadata":{"id":"VcQufrWvfY1J"},"source":["## Google Colab setup (don't run locally)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ld1DehvfmVE","outputId":"2ada0388-8cca-40b6-faa8-4be395a173d4","executionInfo":{"status":"ok","timestamp":1745372328082,"user_tz":240,"elapsed":3269,"user":{"displayName":"Jade S","userId":"08462326512389871838"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","import shutil\n","\n","# Mount drive to colab\n","drive.mount('/content/drive', force_remount=True)\n","\n","# setting up paths\n","path_to_project_files = '/content/drive/MyDrive/School/Homework/Spring2025/DL/Project/'\n","existing = os.path.join(path_to_project_files, 'kaggle.json')\n","path_to_colab_utils = '/root/.kaggle'\n","target = os.path.join(path_to_colab_utils, 'kaggle.json')\n","\n","# move the key to the colab root\n","os.makedirs(path_to_colab_utils, exist_ok=True)\n","shutil.copy(existing, target)\n","os.chmod(target, 600)\n","\n","# download the data into /content (which is temporary)\n","#!unzip '/content/drive/MyDrive/School/Homework/Spring2025/DL/Project/leafsnap_fixed_set.zip' -d '/content/leafsnap-dataset/'\n","#!kaggle datasets download -d xhlulu/leafsnap-dataset -p /content --unzip"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Pojg5BTgiA2x","executionInfo":{"status":"ok","timestamp":1745372328085,"user_tz":240,"elapsed":2,"user":{"displayName":"Jade S","userId":"08462326512389871838"}}},"outputs":[],"source":["import sys\n","\n","# Edit this path to where you've uploaded the repo files, so the imports work.\n","sys.path.append('/content/drive/MyDrive/School/Homework/Spring2025/DL/Project/')"]},{"cell_type":"markdown","metadata":{"id":"VZ6t5flAg_L3"},"source":["## Library Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"c2wCOwrEg_L7","executionInfo":{"status":"ok","timestamp":1745372338144,"user_tz":240,"elapsed":10058,"user":{"displayName":"Jade S","userId":"08462326512389871838"}}},"outputs":[],"source":["from autoencoder import *\n","from dataloader import *\n","from cnn import *\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import v2\n","import os"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_vHlyC5Vg_L8","executionInfo":{"status":"ok","timestamp":1745372338157,"user_tz":240,"elapsed":9,"user":{"displayName":"Jade S","userId":"08462326512389871838"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def showTensorInNotebook(tensor):\n","    \"\"\"\n","    This takes a (3[RGB], H, W) tensor in R[0.0, 1.0] and displays it with matplotlib.\n","    \"\"\"\n","    image = tensor.detach().cpu().numpy().transpose(1,2,0) # move the channel axis to the end, because PIL and matplotlib hate each other\n","    plt.imshow(image)\n","    plt.axis('off')\n","    plt.show()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1745372338204,"user":{"displayName":"Jade S","userId":"08462326512389871838"},"user_tz":240},"id":"qlvsRzNOrXsT","outputId":"e4fea87d-3eb5-4ffc-dbeb-40070a246451"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')"]},{"cell_type":"markdown","metadata":{"id":"B17GtXW4g_L9"},"source":["## Building the data loader"]},{"cell_type":"markdown","metadata":{"id":"Snd7PueTg_L9"},"source":["These are transforms that allow us to ingest the image tensors with some extra confusion at training time. `processor` makes the data loader spit out tensors, and `noiser` adds Gaussian noise."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZBLCG5lig_L9","executionInfo":{"status":"ok","timestamp":1745372338228,"user_tz":240,"elapsed":2,"user":{"displayName":"Jade S","userId":"08462326512389871838"}}},"outputs":[],"source":["# This just processes the images.\n","NOISE_RATIO = 0.1\n","H, W = 256, 256\n","\n","processor = v2.Compose([\n","    v2.PILToTensor(), # the LeafsnapDataset class gives PIL Images, convert to torch Tensor\n","    lambda x: v2.functional.crop(x, 0, 0, 600, 600),\n","    v2.Resize((H, W)), # resize\n","    v2.RandomHorizontalFlip(),\n","    v2.RandomVerticalFlip(),\n","    v2.RandomRotation(degrees=(-45, 45)),\n","    lambda x: x / 255.0, # convert N[0, 255] to R[0.0, 1.0]\n","    #lambda x: torch.clip(x + NOISE_RATIO*torch.randn_like(x), 0.0, 1.0), # add noise\n","])\n","\n","lab_processor = v2.Compose([\n","    v2.PILToTensor(), # the LeafsnapDataset class gives PIL Images, convert to torch Tensor\n","    lambda x: v2.functional.crop(x, 0, 0, 600, 600),\n","    v2.Resize((H, W)), # resize\n","    v2.RandomHorizontalFlip(),\n","    v2.RandomVerticalFlip(),\n","    v2.RandomRotation(degrees=(-45, 45)),\n","    lambda x: x / 255.0, # convert N[0, 255] to R[0.0, 1.0]\n","    #lambda x: torch.clip(x + NOISE_RATIO*torch.randn_like(x), 0.0, 1.0), # add noise\n","])\n","\n","test_processor = v2.Compose([\n","    v2.PILToTensor(), # the LeafsnapDataset class gives PIL Images, convert to torch Tensor\n","    lambda x: v2.functional.crop(x, 0, 0, 600, 600),\n","    v2.Resize((H, W)), # resize\n","    lambda x: x / 255.0, # convert N[0, 255] to R[0.0, 1.0]\n","])\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"frUcSFc1g_L-","executionInfo":{"status":"ok","timestamp":1745372339593,"user_tz":240,"elapsed":1364,"user":{"displayName":"Jade S","userId":"08462326512389871838"}}},"outputs":[],"source":["BATCH_SIZE = 32\n","\n","root_directory = os.path.join(os.getcwd(), 'leafsnap-dataset') # you make need to edit this path to work, though, it works on Colab by default and works locally if you keep the dataset at the root of the repo\n","train_image_paths_file = os.path.join(path_to_project_files, \"train.txt\")\n","train_dataset = LeafsnapDataset(train_image_paths_file, root_directory, use_segmented=False, source=\"both\", expand_lab=True, transform=processor, lab_transform=lab_processor)\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","validation_image_paths_file = os.path.join(path_to_project_files, \"validation.txt\")\n","validation_dataset = LeafsnapDataset(validation_image_paths_file, root_directory, use_segmented=False, source=\"both\", expand_lab=True, transform=processor, lab_transform=lab_processor)\n","validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","test_image_paths_file = os.path.join(path_to_project_files, \"test.txt\")\n","test_dataset = LeafsnapDataset(test_image_paths_file, root_directory, use_segmented=False, source=\"both\", transform=test_processor, lab_transform=test_processor)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"FaLBO6h8g_L-"},"source":["## Training the Convolutional Neural Network (CNN)"]},{"cell_type":"markdown","metadata":{"id":"WK2Yd5Nwg_L_"},"source":["This model uses a modified version of ResNet from Homework 2. It specifically is a version of ResNet34, with kernel size raised to 5, and skip layers at sizes 32, 64, and 128. Images have been downscaled to 128x128, and the segmentation image is used as a 4th layer, resulting in an input size of 4x128x128."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"693oRWNdg_L_","outputId":"e781edfc-f854-4e3c-c58f-7f553ef913dc"},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10:  24%|██▍       | 183/769 [04:19<14:08,  1.45s/it, batch=183/769, loss=5.23]"]}],"source":["model = resnet(3, 185, device=device)\n","#model.load_state_dict(torch.load(os.path.join(path_to_project_files, 'cnn_model.pth'), weights_only=True))\n","\n","train_resnet_model(model, train_dataloader, validation_dataloader, 10, .001, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQzs2bGwg_MA"},"outputs":[],"source":["torch.save(model.state_dict(), path_to_project_files + \"cnn_model.pth\")"]},{"cell_type":"markdown","metadata":{"id":"rVA0iHLeg_MA"},"source":["## Testing the CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"whBbgkVbg_MB"},"outputs":[],"source":["correct_top1 = 0\n","correct_top5 = 0\n","total = 0\n","\n","model.to(device)\n","with torch.no_grad(): # No gradients needed for evaluation\n","    model.eval()\n","    for inputs, labels in test_dataloader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","\n","        # Top-1 Accuracy\n","        _, predicted = torch.max(outputs, 1)\n","        correct_top1 += (predicted == labels).sum().item()\n","\n","        # Top-5 Accuracy\n","        top5_preds = torch.topk(outputs, 5, dim=1).indices\n","        correct_top5 += torch.sum(top5_preds.eq(labels.view(-1, 1))).item()\n","\n","        total += labels.size(0)\n","\n","# Compute accuracies\n","top1_accuracy = 100 * correct_top1 / total\n","top5_accuracy = 100 * correct_top5 / total\n","\n","print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\n","print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"b_t7sGbPWNOn"},"source":["After various tweaks, I'm very happy with the current training accuracy of the CNN model, especially for the first check-in. Running at a 90% Top-5 accuracy is excellent, although there is certainly some more hyperparameter tweaking to be done. I may also test changing the model's structure, adding techniques such as dropout that have been used in other models for similar purposes. I would like to reach 80% Top-1 accuracy by the end."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"}},"nbformat":4,"nbformat_minor":0}